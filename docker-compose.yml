services:
  # 1. The AI Engine
  ollama:
    image: ollama/ollama:latest
    container_name: phishnet-ollama
    volumes:
      - ollama_storage:/root/.ollama
    ports:
      - "11434:11434"
    healthcheck:
      test: [ "CMD", "ollama", "list" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # 2. The Auto-Loader (New)
  ollama-pull-model:
    image: curlimages/curl:latest
    container_name: phishnet-model-loader
    depends_on:
      ollama:
        condition: service_healthy
    # This pulls Llama 3.2 1B (the most efficient for low RAM)
    command: >
      http://ollama:11434/api/pull -d '{"name": "llama3.2:1b"}'

  # 3. The Backend (renamed from 'backend' to 'api' to match existing structure)
  api:
    build:
      context: ./apps/api
    container_name: phishnet-api
    # Force Docker to read .env
    env_file: .env
    depends_on:
      - ollama
      - runner
    environment:
      # Swapping Postgres for SQLite
      - DATABASE_URL=sqlite:///data/phishnet.db
      - OLLAMA_BASE_URL=http://ollama:11434
      - ARTIFACT_DIR=/artifacts
      - RUNNER_BASE_URL=http://runner:7070
    volumes:
      - ./artifacts:/artifacts
      # Volume for SQLite DB persistence
      - ./apps/api/data:/app/data
    ports:
      - "8000:8000"

  # 4. The Runner (Kept as is, just ensuring it connects to API network/logic if needed)
  runner:
    build:
      context: ./apps/runner
    container_name: phishnet-runner
    environment:
      OUT_DIR: /out
    volumes:
      - ./artifacts:/out

  # 5. The Frontend (renamed from 'frontend' to 'web' to match existing structure)
  web:
    build:
      context: ./apps/web
    container_name: phishnet-ui
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_BASE=http://localhost:8000
    depends_on:
      - api

volumes:
  ollama_storage:
